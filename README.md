# Character-Level Language Modeling with GPT 

![image](https://github.com/RaahimNadeem/Character-Level-Language-Modeling-with-GPT-/assets/114340940/5d60349e-8947-4572-9e81-200d3070d527)

Drawing inspiration from the influential paper titled "Attention is All You Need" and the groundbreaking GPT-2/GPT-3 models developed by OpenAI, I set out to train a character-level language model. To achieve this, I utilized a fascinating dataset known as "Tiny Shakespeare," which comprises 40,000 lines from a variety of Shakespeare's plays.

My primary goal was to gain a deeper understanding of the underlying mechanisms employed by models like ChatGPT. To accomplish this, I developed a Generatively Pretrained Transformer (GPT) specifically designed to predict the next most likely character based on the preceding context. In crafting this transformer, I took inspiration from Andrej Karpathy's open-source NanoGPT, which served as a valuable resource.

One of the notable advantages of this project is its flexibility. It can be adapted to accommodate any dataset, thereby facilitating a better grasp of the inner workings of ChatGPT. By training a transformer-based language model, I was able to acquire insights into the mechanisms that drive coherent and contextually relevant responses in chat-based models.

It's important to note that due to limited computational resources at hand and the scale of the training, the outcomes of this project, while possessing a Shakespearean flair, may seem nonsensical at times. However, with increased GPU and CPU capabilities, the model's coherence and overall quality can be significantly enhanced.

For those interested in exploring the foundational papers that served as the bedrock for this project, I have provided the links below:

-  "Attention is All You Need": https://lnkd.in/d-_rYsR4
-  "Dropout: A Simple Way to Prevent Neural Networks from Overfitting": https://lnkd.in/d4_G4Pu3

Overall, this endeavor into language modeling has been a fascinating journey. It has allowed me to delve into the intricate workings of models like ChatGPT and gain valuable insights into the world of natural language generation.
